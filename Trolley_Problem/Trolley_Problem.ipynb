{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the main images\n",
    "\n",
    "inputImage = cv2.imread('Trolley_problem.jpg')\n",
    "\n",
    "# Read the templates\n",
    "\n",
    "tramTemplate = cv2.imread('tram1.jpg')\n",
    "humanTemplate = cv2.imread('h1.jpg')\n",
    "humanSlantTemplate = cv2.imread('hr1.jpg')\n",
    "leverTemplate = cv2.imread('lever.jpg')\n",
    "leverFlipTemplate = cv2.flip(leverTemplate, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store width and height of the templates in w and h\n",
    "\n",
    "h1, w1, _ = tramTemplate.shape\n",
    "h2, w2, _= humanTemplate.shape\n",
    "h3, w3, _ = leverTemplate.shape\n",
    "h4, w4, _ = humanSlantTemplate.shape\n",
    "h5, w5, _ = leverFlipTemplate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform match operations.\n",
    "\n",
    "tramResult = cv2.matchTemplate(inputImage, tramTemplate, cv2.TM_CCOEFF_NORMED)\n",
    "humanResult = cv2.matchTemplate(inputImage, humanTemplate, cv2.TM_CCOEFF_NORMED)\n",
    "leverResult = cv2.matchTemplate(inputImage, leverTemplate, cv2.TM_CCOEFF_NORMED)\n",
    "humanSlantResult = cv2.matchTemplate(inputImage, humanSlantTemplate, cv2.TM_CCOEFF_NORMED)\n",
    "leverFlipResult = cv2.matchTemplate(inputImage, leverFlipTemplate, cv2.TM_CCOEFF_NORMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.75 # Specify a threshold\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "object_detected = []\n",
    "total_objects = 0\n",
    "objects= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the coordinates of matched area in a numpy array\n",
    "\n",
    "loc_tram = np.where(tramResult >= threshold)\n",
    "loc_human = np.where(humanResult >= threshold)\n",
    "loc_lever = np.where(leverResult >= threshold)\n",
    "loc_human_slant = np.where(humanSlantResult >= 0.80)\n",
    "loc_lever_flip = np.where(leverFlipResult >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Detection\n",
    "\n",
    "edges = cv2.Canny(inputImage, 150, 200, apertureSize=3)\n",
    "\n",
    "minLineLength = 30  # Minimum length of line. Line segments shorter than this are rejected.\n",
    "maxLineGap = 5  # Maximum allowed gap between line segments to treat them as single line\n",
    "lines = cv2.HoughLinesP(image=edges,\n",
    "                        rho=cv2.HOUGH_PROBABILISTIC,\n",
    "                        theta=np.pi / 180,\n",
    "                        threshold =30,\n",
    "                        minLineLength = minLineLength,\n",
    "                        maxLineGap = maxLineGap)\n",
    "\n",
    "trackLoc = []\n",
    "\n",
    "for x in range(0, len(lines)):\n",
    "    for x1, y1, x2, y2 in lines[x]:\n",
    "        pts = np.array([[x1, y1], [x2, y2]], dtype=np.int32)\n",
    "        cv2.polylines(img=inputImage , pts=[pts], isClosed=True, color=(0, 255, 0))\n",
    "        # trackLoc.append(pts.tolist())\n",
    "        objects = \"Tracks\"\n",
    "\n",
    "object_detected.append(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a rectangle around the matched region.\n",
    "\n",
    "# for pt in Zip(*loc[::-1]): this command is for the points which have values greater than threshold.\n",
    "# zip is container of all such points, and it will iterate to all such points and draw rectangle around this closed entity\n",
    "\n",
    "# detect tram\n",
    "\n",
    "for tram_pt in zip(*loc_tram[::-1]):\n",
    "    cv2.rectangle(inputImage, tram_pt, (tram_pt[0] + w1, tram_pt[1] + h1), (0, 255, 255), 1)\n",
    "    # tramLoc = (tram_pt, (tram_pt[0] + w1, tram_pt[1] + h1))\n",
    "    cv2.putText(inputImage, \"Tram Detected\", (200, 50), font, 0.5, 255)\n",
    "    objects = \"Tram\"\n",
    "\n",
    "object_detected.append(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect humans\n",
    "\n",
    "for human_pt in zip(*loc_human[::-1]):\n",
    "    cv2.rectangle(inputImage, human_pt, (human_pt[0] + w2, human_pt[1] + h2), (0, 0, 255), 1)\n",
    "    # humanLoc = (human_pt, (human_pt[0] + w2, human_pt[1] + h2))\n",
    "    cv2.putText(inputImage, \"Humans Detected\", (800, 50), font, 0.5, 255)\n",
    "    objects = \"Human\"\n",
    "\n",
    "for human_slant_pt in zip(*loc_human_slant[::-1]):\n",
    "    cv2.rectangle(inputImage, human_slant_pt, (human_slant_pt[0] + w4, human_slant_pt[1] + h4), (0, 0, 255), 1)\n",
    "\n",
    "object_detected.append(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect lever\n",
    "\n",
    "for lever_pt in zip(*loc_lever[::-1]):\n",
    "    cv2.rectangle(inputImage, lever_pt, (lever_pt[0] + w3, lever_pt[1] + h3), (255, 255, 0), 1)\n",
    "    # leverLoc = (lever_pt, (lever_pt[0] + w3, lever_pt[1] + h3))\n",
    "    cv2.putText(inputImage, \"Lever Detected\", (480, 50), font, 0.5, 255)\n",
    "    objects = \"Lever\"\n",
    "\n",
    "for lever_flip_pt in zip(*loc_lever_flip[::-1]):\n",
    "    cv2.rectangle(inputImage, lever_flip_pt, (lever_flip_pt[0] + w5, lever_flip_pt[1] + h5), (255, 255, 0), 1)\n",
    "    cv2.putText(inputImage, \"Lever Detected\", (480, 50), font, 0.5, 255)\n",
    "\n",
    "object_detected.append(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(r'outputImage.jpg', inputImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to json file\n",
    "\n",
    "obj = []\n",
    "for i, img_obj in enumerate(object_detected):\n",
    "    obj.append((f'object {i}', img_obj))\n",
    "\n",
    "data=dict(obj)\n",
    "data['Total Objects in the image'] = len(data)\n",
    "\n",
    "with open('result.json', 'w') as file:\n",
    "    file.write(json.dumps(data, indent=4, separators=(',', ': ')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
